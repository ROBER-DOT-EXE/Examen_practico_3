{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9bfa09e-f54f-4fa5-bb53-85792757b7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Preparar el fichero `orders_data.parquet` de modo que pueda ser usado para contruir un 'forecasting model'.  \n",
    "- Limpiar la dataset para que cumpla los requerimientos del equipo de Data y Machine Learning.  \n",
    "- Guardar el archivo actualizado (limpio) como `orders_data_clean.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2991b69f-f0d9-4f7b-8891-71c486e6b7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  \n",
    "![](https://community.cloud.databricks.com/files/EP_3/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e722f5a-d92b-4210-80ea-6fff061b7081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Como ingeniero de datos de una empresa de comercio electrónico llamada Voltmart, un equipo de aprendizaje automático le ha pedido que limpie los datos que contienen información sobre los pedidos realizados el año pasado. Tienen previsto utilizar estos datos depurados para crear un modelo de previsión de la demanda (Forecasting Model). Para ello, han compartido sus requisitos sobre el formato de tabla de salida deseado.\n",
    "\n",
    "Un analista ha compartido un archivo parquet llamado `orders_data.parquet` para que usted los limpie y los preprocese.\n",
    "\n",
    "A continuación puede ver el esquema del conjunto de datos junto con los requisitos de limpieza de los perezosos analistas de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a793c3b-1d16-4ba8-98b6-1ccb3be7d1b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `orders_data.parquet`\n",
    "\n",
    "| column | data type | description | cleaning requirements | \n",
    "|--------|-----------|-------------|-----------------------|\n",
    "| `order_date` | `timestamp` | Date and time when the order was made | _Modify: Remove orders placed between 12am and 5am (inclusive); convert from timestamp to date_ |\n",
    "| `time_of_day` | `string` | Period of the day when the order was made | _New column containing (lower bound inclusive, upper bound exclusive): \"morning\" for orders placed 5-12am, \"afternoon\" for orders placed 12-6pm, and \"evening\" for 6-12pm_ |\n",
    "| `order_id` | `long` | Order ID | _N/A_ |\n",
    "| `product` | `string` | Name of a product ordered | _Remove rows containing \"TV\" as the company has stopped selling this product; ensure all values are lowercase_ |\n",
    "| `product_ean` | `double` | Product ID | _N/A_ |\n",
    "| `category` | `string` | Broader category of a product | _Ensure all values are lowercase_ |\n",
    "| `purchase_address` | `string` | Address line where the order was made (\"House Street, City, State Zipcode\") | _N/A_ |\n",
    "| `purchase_state` | `string` | US State of the purchase address | _New column containing: the State that the purchase was ordered from_ |\n",
    "| `quantity_ordered` | `long` | Number of product units ordered | _N/A_ |\n",
    "| `price_each` | `double` | Price of a product unit | _N/A_ |\n",
    "| `cost_price` | `double` | Cost of production per product unit | _N/A_ |\n",
    "| `turnover` | `double` | Total amount paid for a product (quantity x price) | _N/A_ |\n",
    "| `margin` | `double` | Profit made by selling a product (turnover - cost) | _N/A_ |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32eb7dfb-bcca-467e-a058-ffc6a8c57cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    SparkSession,\n",
    "    types,\n",
    "    functions as F,\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Caso_1')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3848b72-2ba5-4779-8813-1b6e7416a775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_id</th>\n",
       "      <th>product</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category</th>\n",
       "      <th>purchase_address</th>\n",
       "      <th>quantity_ordered</th>\n",
       "      <th>price_each</th>\n",
       "      <th>cost_price</th>\n",
       "      <th>turnover</th>\n",
       "      <th>margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-22 21:25:00</td>\n",
       "      <td>141234</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>5.638009e+12</td>\n",
       "      <td>Vêtements</td>\n",
       "      <td>944 Walnut St, Boston, MA 02215</td>\n",
       "      <td>1</td>\n",
       "      <td>700.00</td>\n",
       "      <td>231.0000</td>\n",
       "      <td>700.00</td>\n",
       "      <td>469.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-28 14:15:00</td>\n",
       "      <td>141235</td>\n",
       "      <td>Lightning Charging Cable</td>\n",
       "      <td>5.563320e+12</td>\n",
       "      <td>Alimentation</td>\n",
       "      <td>185 Maple St, Portland, OR 97035</td>\n",
       "      <td>1</td>\n",
       "      <td>14.95</td>\n",
       "      <td>7.4750</td>\n",
       "      <td>14.95</td>\n",
       "      <td>7.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-17 13:33:00</td>\n",
       "      <td>141236</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>2.113973e+12</td>\n",
       "      <td>Vêtements</td>\n",
       "      <td>538 Adams St, San Francisco, CA 94016</td>\n",
       "      <td>2</td>\n",
       "      <td>11.99</td>\n",
       "      <td>5.9950</td>\n",
       "      <td>23.98</td>\n",
       "      <td>11.9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-05 20:33:00</td>\n",
       "      <td>141237</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>3.069157e+12</td>\n",
       "      <td>Sports</td>\n",
       "      <td>738 10th St, Los Angeles, CA 90001</td>\n",
       "      <td>1</td>\n",
       "      <td>149.99</td>\n",
       "      <td>97.4935</td>\n",
       "      <td>149.99</td>\n",
       "      <td>52.4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-25 11:59:00</td>\n",
       "      <td>141238</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>9.692681e+12</td>\n",
       "      <td>Électronique</td>\n",
       "      <td>387 10th St, Austin, TX 73301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>5.9950</td>\n",
       "      <td>11.99</td>\n",
       "      <td>5.9950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_date</th>\n      <th>order_id</th>\n      <th>product</th>\n      <th>product_id</th>\n      <th>category</th>\n      <th>purchase_address</th>\n      <th>quantity_ordered</th>\n      <th>price_each</th>\n      <th>cost_price</th>\n      <th>turnover</th>\n      <th>margin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-01-22 21:25:00</td>\n      <td>141234</td>\n      <td>iPhone</td>\n      <td>5.638009e+12</td>\n      <td>Vêtements</td>\n      <td>944 Walnut St, Boston, MA 02215</td>\n      <td>1</td>\n      <td>700.00</td>\n      <td>231.0000</td>\n      <td>700.00</td>\n      <td>469.0000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-01-28 14:15:00</td>\n      <td>141235</td>\n      <td>Lightning Charging Cable</td>\n      <td>5.563320e+12</td>\n      <td>Alimentation</td>\n      <td>185 Maple St, Portland, OR 97035</td>\n      <td>1</td>\n      <td>14.95</td>\n      <td>7.4750</td>\n      <td>14.95</td>\n      <td>7.4750</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-01-17 13:33:00</td>\n      <td>141236</td>\n      <td>Wired Headphones</td>\n      <td>2.113973e+12</td>\n      <td>Vêtements</td>\n      <td>538 Adams St, San Francisco, CA 94016</td>\n      <td>2</td>\n      <td>11.99</td>\n      <td>5.9950</td>\n      <td>23.98</td>\n      <td>11.9900</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-01-05 20:33:00</td>\n      <td>141237</td>\n      <td>27in FHD Monitor</td>\n      <td>3.069157e+12</td>\n      <td>Sports</td>\n      <td>738 10th St, Los Angeles, CA 90001</td>\n      <td>1</td>\n      <td>149.99</td>\n      <td>97.4935</td>\n      <td>149.99</td>\n      <td>52.4965</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-01-25 11:59:00</td>\n      <td>141238</td>\n      <td>Wired Headphones</td>\n      <td>9.692681e+12</td>\n      <td>Électronique</td>\n      <td>387 10th St, Austin, TX 73301</td>\n      <td>1</td>\n      <td>11.99</td>\n      <td>5.9950</td>\n      <td>11.99</td>\n      <td>5.9950</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.parquet('/FileStore/Caso_1/orders_data.parquet')\n",
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83685abc-c333-4fb3-9417-e2cbfc7acba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Respuestas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9ca43b-a21a-47e2-a533-352c6c8cde6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1. Modify: Remove orders placed between 12am and 5am (inclusive); convert from timestamp to date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4a7120-a7d6-45b6-bf2c-8aff044965a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- order_date: timestamp (nullable = true)\n |-- order_id: long (nullable = true)\n |-- product: string (nullable = true)\n |-- product_id: double (nullable = true)\n |-- category: string (nullable = true)\n |-- purchase_address: string (nullable = true)\n |-- quantity_ordered: long (nullable = true)\n |-- price_each: double (nullable = true)\n |-- cost_price: double (nullable = true)\n |-- turnover: double (nullable = true)\n |-- margin: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Muestro el esquema del df original para comprobar que order_date es timestamp\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18bcc24-d406-4632-b4db-818f795aba32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- order_date: timestamp (nullable = true)\n |-- order_id: long (nullable = true)\n |-- product: string (nullable = true)\n |-- product_id: double (nullable = true)\n |-- category: string (nullable = true)\n |-- purchase_address: string (nullable = true)\n |-- quantity_ordered: long (nullable = true)\n |-- price_each: double (nullable = true)\n |-- cost_price: double (nullable = true)\n |-- turnover: double (nullable = true)\n |-- margin: double (nullable = true)\n |-- order_date_formateado: date (nullable = true)\n\n+-------------------+--------+--------------------+-----------------+------------+--------------------+----------------+----------+----------+--------+-------+---------------------+\n|         order_date|order_id|             product|       product_id|    category|    purchase_address|quantity_ordered|price_each|cost_price|turnover| margin|order_date_formateado|\n+-------------------+--------+--------------------+-----------------+------------+--------------------+----------------+----------+----------+--------+-------+---------------------+\n|2023-01-22 21:25:00|  141234|              iPhone|5.638008983335E12|   Vêtements|944 Walnut St, Bo...|               1|     700.0|     231.0|   700.0|  469.0|           2023-01-22|\n|2023-01-28 14:15:00|  141235|Lightning Chargin...|5.563319511488E12|Alimentation|185 Maple St, Por...|               1|     14.95|     7.475|   14.95|  7.475|           2023-01-28|\n|2023-01-17 13:33:00|  141236|    Wired Headphones| 2.11397339522E12|   Vêtements|538 Adams St, San...|               2|     11.99|     5.995|   23.98|  11.99|           2023-01-17|\n|2023-01-05 20:33:00|  141237|    27in FHD Monitor|3.069156759167E12|      Sports|738 10th St, Los ...|               1|    149.99|   97.4935|  149.99|52.4965|           2023-01-05|\n|2023-01-25 11:59:00|  141238|    Wired Headphones|9.692680938163E12|Électronique|387 10th St, Aust...|               1|     11.99|     5.995|   11.99|  5.995|           2023-01-25|\n+-------------------+--------+--------------------+-----------------+------------+--------------------+----------------+----------+----------+--------+-------+---------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, to_date\n",
    "\n",
    "# Creo un dataframe con el archivo\n",
    "df = spark.read.parquet('/FileStore/Caso_1/orders_data.parquet')\n",
    "\n",
    "# Uso el df creado antes para filtrar en la sección de la hora del timestamp de los valores de la columna \"order_date\" los estén entre 0 y 5 que sería entre las 12am y las 5am. Una vez filtrado, creo una columna llamada \"order_date\" también que tiene el contenido en formato date de la columna order_date del df original. Con estos cambios creo el df resultante \"df_filtrado\"\n",
    "df_filtrado = df.filter(~((hour('order_date') >= 0) & (hour('order_date') <= 5))) \\\n",
    "    .withColumn('order_date_formateado', to_date('order_date'))\n",
    "\n",
    "# Muestro el schema para que se vea que la columna order_date es de tipo \"date\" e imprimo las 5 primeras filas\n",
    "df_filtrado.printSchema()\n",
    "df_filtrado.show(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af688b39-614f-4620-bd5a-b7bae774bc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2. New column containing (lower bound inclusive, upper bound exclusive): \"morning\" for orders placed 5-12am, \"afternoon\" for orders placed 12-6pm, and \"evening\" for 6-12pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375d45cc-5fa2-41ae-adc5-3d1237dc112f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n|         order_date|periodo_de_tiempo|\n+-------------------+-----------------+\n|2023-01-22 21:25:00|          Evening|\n|2023-01-28 14:15:00|        Afternoon|\n|2023-01-17 13:33:00|        Afternoon|\n|2023-01-05 20:33:00|          Evening|\n|2023-01-25 11:59:00|          Morning|\n|2023-01-29 20:22:00|          Evening|\n|2023-01-26 12:16:00|        Afternoon|\n|2023-01-05 12:04:00|        Afternoon|\n|2023-01-01 10:30:00|          Morning|\n|2023-01-22 21:20:00|          Evening|\n|2023-01-07 11:29:00|          Morning|\n|2023-01-31 10:12:00|          Morning|\n|2023-01-09 18:57:00|          Evening|\n|2023-01-25 19:19:00|          Evening|\n|2023-01-03 21:54:00|          Evening|\n|2023-01-05 17:20:00|        Afternoon|\n|2023-01-10 11:20:00|          Morning|\n|2023-01-24 08:13:00|          Morning|\n|2023-01-30 09:28:00|          Morning|\n|2023-01-08 11:51:00|          Morning|\n+-------------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, when\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Creo una función que obtiene un dataframe como argumento y que devuelve un dataframe con una columna \"periodo_de_tiempo\" que tiene los valores Morning, Afternoon y Evening dependiendo de la hora que detecte en valor de la columna order_date\n",
    "def agregar_periodo_de_tiempo(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumn(\n",
    "        'periodo_de_tiempo',\n",
    "        when((hour('order_date') >= 5) & (hour('order_date') < 12), 'Morning')\n",
    "        .when((hour('order_date') >= 12) & (hour('order_date') < 18), 'Afternoon')\n",
    "        .otherwise('Evening')\n",
    "    )\n",
    "\n",
    "#Creo un dataframe nuevo en base al que me devuelve la función que he creado arriba y después lo muestro (primeras 20 filas por defecto y sólo las columnas order_date y periodo_de_tiempo)\n",
    "df_filtrado = agregar_periodo_de_tiempo(df_filtrado)\n",
    "df_filtrado.select(\"order_date\", \"periodo_de_tiempo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de882743-3a8e-48e5-81b6-b18b033da1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3. Remove rows containing \"TV\" as the company has stopped selling this product; ensure all values are lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645805b4-396f-4dd5-8238-3081929226fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|             product|\n+--------------------+\n|              iphone|\n|lightning chargin...|\n|    wired headphones|\n|    27in fhd monitor|\n|    wired headphones|\n|aaa batteries (4-...|\n|27in 4k gaming mo...|\n|usb-c charging cable|\n|bose soundsport h...|\n|apple airpods hea...|\n|apple airpods hea...|\n|  macbook pro laptop|\n|aaa batteries (4-...|\n|    27in fhd monitor|\n|    27in fhd monitor|\n|     vareebadd phone|\n|apple airpods hea...|\n|usb-c charging cable|\n|aaa batteries (4-...|\n|usb-c charging cable|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "# Creo una función que toma un df como argumento y devuelve otro en el que no hay filas con el string \"tv\" dentro de la columna \"product\".\n",
    "# Esto lo condigue cambiando la columna \"product\" por sí misma pero con los valores en minúsculas y después filtrando las que contiene el string \"tv\"\n",
    "def quitar_tv(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumn('product', lower(col('product'))) \\\n",
    "        .filter(~col('product').contains('tv')) \n",
    "\n",
    "# Asigno el df que devuelve la función a uno nuevo que luego muestro por pantalla (sólo la columna product)\n",
    "df_sin_tv = quitar_tv(df_filtrado)\n",
    "df_sin_tv.select(\"product\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f3cc87-4b9d-4a81-b3d6-e2a0f0af1fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4. Ensure all values are lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e49653-65e4-445e-995f-9c67489a2248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|    category|\n+------------+\n|   vêtements|\n|alimentation|\n|   vêtements|\n|      sports|\n|électronique|\n|alimentation|\n|   vêtements|\n|   vêtements|\n|électronique|\n|électronique|\n|   vêtements|\n|   vêtements|\n|   vêtements|\n|   vêtements|\n|alimentation|\n|alimentation|\n|alimentation|\n|      sports|\n|électronique|\n|alimentation|\n+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Creo un dataframe nuevo partiendo del anterior al que he crado una columna que susituye a la de categoría ya que al tener el mismo nombre pyspark lo hace así por defecto, pero esta nueva columna tiene todos sus valores en minúsculas.\n",
    "df_category_minusculas = df_sin_tv.withColumn('category', lower(col('category')))\n",
    "\n",
    "# Muestro sólo la columna categoría\n",
    "df_category_minusculas.select(\"category\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e15d85b-d5e3-474e-9d85-8a200d10b7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5. New column containing: the State that the purchase was ordered from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8229ca05-d7f2-47bb-a453-4b4cbbfc698d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+------+\n|purchase_address                       |estado|\n+---------------------------------------+------+\n|944 Walnut St, Boston, MA 02215        |MA    |\n|185 Maple St, Portland, OR 97035       |OR    |\n|538 Adams St, San Francisco, CA 94016  |CA    |\n|738 10th St, Los Angeles, CA 90001     |CA    |\n|387 10th St, Austin, TX 73301          |TX    |\n|775 Willow St, San Francisco, CA 94016 |CA    |\n|979 Park St, Los Angeles, CA 90001     |CA    |\n|181 6th St, San Francisco, CA 94016    |CA    |\n|867 Willow St, Los Angeles, CA 90001   |CA    |\n|657 Johnson St, San Francisco, CA 94016|CA    |\n|492 Walnut St, San Francisco, CA 94016 |CA    |\n|322 6th St, San Francisco, CA 94016    |CA    |\n|618 7th St, Los Angeles, CA 90001      |CA    |\n|512 Wilson St, San Francisco, CA 94016 |CA    |\n|440 Cedar St, Portland, OR 97035       |OR    |\n|471 Center St, Los Angeles, CA 90001   |CA    |\n|414 Walnut St, Boston, MA 02215        |MA    |\n|220 9th St, Los Angeles, CA 90001      |CA    |\n|238 Sunset St, Seattle, WA 98101       |WA    |\n|764 11th St, Los Angeles, CA 90001     |CA    |\n+---------------------------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Creo un df nuevo con una columna nueva partiendo del df anterior. Esta columna se crea obteniendo las dos primeras letras mayúsculas que se encuentren al revisar el string de \"purchase_adress\" de manera inversa, es decir, de derecha a izquierda.\n",
    "df_con_estados = df_category_minusculas.withColumn('estado', \n",
    "    regexp_extract(col('purchase_address'), r'.*,\\s+([A-Z]{2}).*', 1)\n",
    ")\n",
    "\n",
    "# Una vez terminado, muestro el nuevo df (sólo las columnas purchase_adress y estado sin abreviar)\n",
    "df_con_estados.select('purchase_address', 'estado').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5ba4d8-7ea3-4460-8f9d-218e7a907231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1bb2c3-6150-4ecc-b4cc-15047f9c680d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 6. Guardar archivo final limpio con nombre `orders_data_clean.parquet` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d3da6-bb40-489b-ac1f-14cbba134831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escribo el contenido del df obtenido en la celda anterior al parquet con el nombre solicitado\n",
    "df_con_estados.write.format(\"parquet\").save(\"/FileStore/Caso_1/output/orders_data_clean.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a6bda0-c061-439a-bb18-83b2c20d5fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5eacdae-eb35-432d-b0b2-700fe2fd44f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7. Exportar archivo limpio en formato CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b0e3ad-09b8-4426-89f7-17bc337b73aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leo el parquet y creo un dataframe con el contenido\n",
    "df = spark.read.parquet(\"/FileStore/Caso_1/orders_data_clean.parquet\")\n",
    "\n",
    "# Creo un csv a partir del df que he obtenido y lo configuro para que use comas como delimitador y que tenga índices, entre otras.\n",
    "df.write.option(\"header\", \"true\") \\\n",
    "       .option(\"delimiter\", \",\") \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .format(\"csv\") \\\n",
    "       .save(\"/FileStore/Caso_1/output/orders_data_clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
